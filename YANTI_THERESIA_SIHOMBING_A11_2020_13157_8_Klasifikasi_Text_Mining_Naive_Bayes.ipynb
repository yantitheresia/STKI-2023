{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yantitheresia/STKI-2023/blob/main/YANTI_THERESIA_SIHOMBING_A11_2020_13157_8_Klasifikasi_Text_Mining_Naive_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQzFvuEXYO29"
      },
      "source": [
        "## Klasifikasi Text Mining menggunakan Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGp1bD18YjLY"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# import sys\n",
        "# sys.path.append('/content/drive/My Drive/Colab Notebooks/AMS2023')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0lyD-xfZQge"
      },
      "outputs": [],
      "source": [
        "# !pip install ekphrasis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2n8VFUcCZWuV"
      },
      "outputs": [],
      "source": [
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    # terms that will be normalized\n",
        "    normalize=['email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'date', 'number'],\n",
        "    # terms that will be annotated\n",
        "    #annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",'emphasis', 'censored'},\n",
        "    annotate={\"hashtag\",\"allcaps\",\"elongated\",\"repeated\",'emphasis','censored'},\n",
        "    fix_html=True,  # fix HTML tokens\n",
        "\n",
        "    # corpus from which the word statistics are going to be used\n",
        "    # for word segmentation\n",
        "    segmenter=\"twitter\",\n",
        "\n",
        "    # corpus from which the word statistics are going to be used\n",
        "    # for spell correction\n",
        "    corrector=\"twitter\",\n",
        "\n",
        "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
        "    spell_correct_elong=False,  # spell correction for elongated words\n",
        "\n",
        "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "    # the tokenizer, should take as input a string and return a list of tokens\n",
        "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "\n",
        "    # list of dictionaries, for replacing tokens extracted from the text,\n",
        "    # with other expressions. You can pass more than one dictionaries.\n",
        "    dicts=[emoticons]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmapjRiEYGPd"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYaLrOg2M5AP"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2i-BhMXXM4FS"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qSlLSEGM7sa"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "classifier_nb = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('model', MultinomialNB()),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brvxUKO0M87t"
      },
      "outputs": [],
      "source": [
        "parameters_nb = {\n",
        "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
        "    'model__alpha': (0.0001, 0.001, 0.1, 1, 10, 100)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEQHo9ZoNACG"
      },
      "outputs": [],
      "source": [
        "classifier_nb = GridSearchCV(classifier_nb, parameters_nb, cv = 3, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y64S8WkyNBoL"
      },
      "outputs": [],
      "source": [
        "classifier_nb.fit(train_x, train_label.ravel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffb4ytJlNC0F"
      },
      "outputs": [],
      "source": [
        "y_pred_nb_train = classifier_nb.predict(train_x)\n",
        "accuracy_nb_train = accuracy_score(train_label, y_pred_nb_train)\n",
        "print(\"Accuracy Training set: \", accuracy_nb_train)\n",
        "\n",
        "y_pred_nb_test = classifier_nb.predict(test_x)\n",
        "accuracy_nb_test = accuracy_score(test_label, y_pred_nb_test)\n",
        "print(\"Accuracy Test set: \", accuracy_nb_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxAILTVzNEeq"
      },
      "outputs": [],
      "source": [
        "recall_nb_train = recall_score(train_label, y_pred_nb_train, average='weighted')\n",
        "print(\"Recall Training set: \", recall_nb_train)\n",
        "\n",
        "recall_nb_test = recall_score(test_label, y_pred_nb_test, average='weighted')\n",
        "print(\"Recall Test set: \", recall_nb_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZtIV1euNF69"
      },
      "outputs": [],
      "source": [
        "precision_nb_train = precision_score(train_label, y_pred_nb_train, average='weighted')\n",
        "print(\"Precision Training set: \", precision_nb_train)\n",
        "\n",
        "precision_nb_test = precision_score(test_label, y_pred_nb_test, average='weighted')\n",
        "print(\"Precision Test set: \", precision_nb_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9gyuq9rNHWf"
      },
      "outputs": [],
      "source": [
        "f1_nb_train = f1_score(train_label, y_pred_nb_train, average='weighted')\n",
        "print(\"F1 Training set: \", f1_nb_train)\n",
        "\n",
        "f1_nb_test = f1_score(test_label, y_pred_nb_test, average='weighted')\n",
        "print(\"F1 Test set: \", f1_nb_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zZjJnhVNIjD"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "sns.heatmap(confusion_matrix(test_label, y_pred_nb_test), annot=True, cmap = 'viridis', fmt='.0f')\n",
        "plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n",
        "plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n",
        "plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCTkN29ENJrT"
      },
      "outputs": [],
      "source": [
        "classifier_nb.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1u8QJO_NLGt"
      },
      "outputs": [],
      "source": [
        "for param_name in sorted(parameters_nb.keys()):\n",
        "    print(\"%s: %r\" % (param_name, classifier_nb.best_params_[param_name]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}